# References and Additional Resources

## Research Papers Referenced in Slides

- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* NAACL. [Link](https://arxiv.org/abs/1810.04805)

- Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). *Language Models are Few-Shot Learners.* NeurIPS. [Link](https://arxiv.org/abs/2005.14165)

- Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). *Improving Language Understanding by Generative Pre-Training.* OpenAI Technical Report. [PDF](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  

- Schoenegger, P., Tuminauskaite, I., Park, P. S., Bastos, R. V. S., & Tetlock, P. E. (2024). *Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy.* Science Advances, 10(45), eadp1528. [Link](https://arxiv.org/abs/2402.19379)

- Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). *Emergent Abilities of Large Language Models.* Transactions on Machine Learning Research (TMLR). [Link](https://arxiv.org/abs/2206.07682)

- Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). *On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?* FAccT. [DOI](https://doi.org/10.1145/3442188.3445922)  

- Dehghani, N., & Levin, M. (2024). *Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence.* arXiv preprint arXiv:2411.15243. [Link](https://arxiv.org/abs/2411.15243)  

- Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* Advances in Neural Information Processing Systems (NeurIPS 2022). [Link](https://arxiv.org/abs/2201.11903)  

- Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., Wang, R., Tu, Z., Mi, H., & Yu, D. (2024). *Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs.* arXiv preprint arXiv:2412.21187. [Link](https://arxiv.org/abs/2412.21187)  

- Wang, Y., & Zhao, Y. (2023). *Metacognitive Prompting Improves Understanding in Large Language Models.* arXiv preprint arXiv:2308.05342. [Link](https://arxiv.org/abs/2308.05342)  

- Das, B. C., Amini, M. H., & Wu, Y. (2024). *Security and Privacy Challenges of Large Language Models: A Survey.* arXiv preprint arXiv:2402.00888. [Link](https://arxiv.org/abs/2402.00888)  

- Deng, Y., Zhang, W., Pan, S. J., & Bing, L. (2023). *Multilingual Jailbreak Challenges in Large Language Models.* arXiv preprint arXiv:2310.06474. [Link](https://arxiv.org/abs/2310.06474)  

- Anil, C., Durmus, E., Panickssery, N., Sharma, M., Benton, J., Kundu, S., Batson, J., Tong, M., Mu, J., Ford, D., Mosconi, F., Agrawal, R., Schaeffer, R., Bashkansky, N., Svenningsen, S., Lambert, M., Radhakrishnan, A., Denison, C., Hubinger, E. J., Bai, Y., Bricken, T., Maxwell, T., Schiefer, N., Sully, J., Tamkin, A., Lanhan, T., Nguyen, K., Korbak, T., Kaplan, J., Ganguli, D., Bowman, S. R., Perez, E., Grosse, R. B., & Duvenaud, D. (2024). *Many-shot Jailbreaking.* Advances in Neural Information Processing Systems (NeurIPS 2024). [Link](https://proceedings.neurips.cc/paper_files/paper/2024/file/ea456e232efb72d261715e33ce25f208-Paper-Conference.pdf)  

- Lindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce, A., Turner, N. L., Citro, C., Abrahams, D., Carter, S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T., McDougall, C., Cunningham, H., Henighan, T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thompson, T. B., Zimmerman, S., Rivoire, K., Conerly, T., Olah, C., & Batson, J. (2025). *On the Biology of a Large Language Model.* Transformer Circuits Thread. [Link](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)  

- Hughes, J., Price, S., Lynch, A., Schaeffer, R., Barez, F., Koyejo, S., Sleight, H., Jones, E., Perez, E., & Sharma, M. (2024). *Best-of-N Jailbreaking.* arXiv preprint arXiv:2412.03556. [Link](https://arxiv.org/abs/2412.03556)  

- Kim, S., Yun, S., Lee, H., Gubri, M., Yoon, S., & Oh, S. J. (2023). *ProPILE: Probing Privacy Leakage in Large Language Models.* arXiv preprint arXiv:2307.01881. [Link](https://arxiv.org/abs/2307.01881)  

- Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., & Glaese, A. (2024). *Deliberative Alignment: Reasoning Enables Safer Language Models.* arXiv preprint arXiv:2412.16339. [Link](https://arxiv.org/abs/2412.16339)  

- Johnson, S. G. B., Karimi, A.-H., Bengio, Y., Chater, N., Gerstenberg, T., Larson, K., Levine, S., Mitchell, M., Rahwan, I., Schölkopf, B., & Grossmann, I. (2024). *Imagining and Building Wise Machines: The Centrality of AI Metacognition.* arXiv preprint arXiv:2411.02478. [Link](https://arxiv.org/abs/2411.02478)  

- Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., & Stoica, I. (2023). *Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.* arXiv preprint arXiv:2306.05685. [Link](https://arxiv.org/abs/2306.05685)

- Li, X., Li, Y., Qiu, L., Joty, S., & Bing, L. (2022). *Evaluating Psychological Safety of Large Language Models.* arXiv preprint arXiv:2212.10529. [Link](https://arxiv.org/abs/2212.10529)

- Tian, J., Hou, J., Wu, Z., Shu, P., Liu, Z., Xiang, Y., Gu, B., Filla, N., Li, Y., Liu, N., Chen, X., Tang, K., Liu, T., & Wang, X. (2024). *Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding.* arXiv preprint arXiv:2401.12983. [Link](https://arxiv.org/abs/2401.12983)

- Luu, R. K., & Buehler, M. J. (2024). *BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-Inspired Materials.* Advanced Science, 11(10), 2306724. [DOI](https://doi.org/10.1002/advs.202306724)  



## Standard Resources
- [DeepLearning.ai](https://www.deeplearning.ai/) – courses on deep learning & NLP  
- [3Blue1Brown](https://www.youtube.com/c/3blue1brown) – intuitive explanations of math concepts  
- [fast.ai](https://www.fast.ai/) – practical deep learning and NLP tutorials  
- [Andrej Karpathy YouTube Channel](https://www.youtube.com/@AndrejKarpathy) – lectures on neural networks and transformers
- [Smol AI newsletter](https://news.smol.ai/)
- [Matthew Berman YouTube Channel](https://www.youtube.com/@matthew_berman) – AI-focused content
- [BlueDot Impact](https://bluedot.org/) & AI Safety Collab - Courses on AI Governance
- YouTube lecture series & Coursera courses on LLMs 
 

## Advanced Resources
- [Neel Nanda YouTube Channel & Blog](https://www.youtube.com/@neelnanda2469) – deep dives into transformer theory and interpretability  
- [Arize AI Community Paper Readings](https://www.deeppapers.dev/) (Deep Papers Podcast) – transformer/LLM research discussion  
- [hu-po YouTube Channel](https://www.youtube.com/@hu-po) – AI/ML research paper discussions  
- [AI Safety Atlas](https://ai-safety-atlas.com/)  
- [Machine Learning Street Talk (MLST) Podcast](https://www.youtube.com/@MachineLearningStreetTalk)  
- Blogs: [LessWrong](https://www.lesswrong.com/), [80,000 Hours](https://80000hours.org/), Substack posts on AI/LLMs  
- Ethical LLM Jailbreaking:  
  - [Gandalf Baseline](https://gandalf.lakera.ai/baseline)  
  - [GPA Challenges](https://gpa.43z.one/)  
  - [Giskard AI Challenges](https://red.giskard.ai/challenges)  
  - [Hack Merlin](https://hackmerlin.io/)  

## Fellowships & Opportunities
- [MATS Research Program](https://www.matsprogram.org/)  
- [ARENA Education](https://www.arena.education/)
- [Cambridge ERA:AI Fellowship](https://erafellowship.org/)
- [Algoverse AI Safety Research Fellowship](https://algoverseairesearch.org/ai-safety-fellowship)  
- [Supervised Program for Alignment Research (SPAR)](https://sparai.org/)  
- [Global AI Safety Fellowship](https://globalaisafetyfellowship.com/)   
- [AI Safety Camp](https://www.aisafety.camp/)
- [AI4H - Innovation Academy Bootcamp](https://ai4h.untapinnovate.com/programs/ai4h-innovation-academy)
- [IAPS AI Policy Fellowship](https://www.iaps.ai/fellowship) 
- [INSAIT Summer Undergraduate Research Fellowship](https://insait.ai/surf/)  
- [Swades.ai Fellowship Program](https://www.swades.ai/fellowship-program)  
- [MITACS Globalink](https://www.mitacs.ca/our-programs/globalink-research-internship-students/)  
- [DAAD-WISE & other DAAD scholarships](https://www.daad.in/en/2023/09/20/applications-invited-working-internships-in-science-and-engineering-wise-2023-24/)  
- [RISE Germany & RISE Professional](https://www.daad.de/rise/en/)  
- Various Summer Research Fellowships – IAS, IITs, IISERs, etc.  

