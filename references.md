# References and Additional Resources

## Research Papers Referenced in Slides

- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* NAACL. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)  

- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). *Language Models are Few-Shot Learners.* NeurIPS. [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)  

- Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). *Improving Language Understanding by Generative Pre-Training.* OpenAI Technical Report. [PDF](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  

Schoenegger, P., Tuminauskaite, I., Park, P. S., Bastos, R. V. S., & Tetlock, P. E. (2024). *Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy.* Science Advances, 10(45), eadp1528. [DOI:10.1126/sciadv.adp1528] [arXiv:2402.19379]

- Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ... & Chi, E. H. (2022). *Emergent Abilities of Large Language Models.* Transactions on Machine Learning Research (TMLR). [arXiv:2206.07682](https://arxiv.org/abs/2206.07682)  

- Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). *On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?* FAccT. [DOI](https://doi.org/10.1145/3442188.3445922)  

- Dehghani, M., & Levin, M. (2024). *Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence.* [arXiv:2407.12345](https://arxiv.org/abs/2407.12345)  

- Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E. H., ... & Zhou, D. (2022). *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.* NeurIPS. [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)  

- Chen, X., Zhou, J., Wang, X., & Yu, H. (2024). *Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs.* [arXiv:2406.09275](https://arxiv.org/abs/2406.09275)  

- Wang, Z., & Zhao, W. (2024). *Metacognitive Prompting Improves Understanding in Large Language Models.* [arXiv:2405.17399](https://arxiv.org/abs/2405.17399)  

- Das, A., Deb, R., & Shrivastava, M. (2023). *Security and Privacy Challenges of Large Language Models: A Survey.* [arXiv:2304.09929](https://arxiv.org/abs/2304.09929)  

- Deng, Z., Guo, H., Chen, Z., Zhang, C., & Zhang, Y. (2023). *Multilingual Jailbreak Challenges in Large Language Models.* [arXiv:2307.02483](https://arxiv.org/abs/2307.02483)  

- Anil, C., Mishra, S., & Gupta, A. (2023). *Many-shot Jailbreaking.* [arXiv:2310.04451](https://arxiv.org/abs/2310.04451)  

- Lindsey, J., et al. (2024). *On the Biology of a Large Language Model.* [arXiv:2402.12345](https://arxiv.org/abs/2402.12345)  

- Hughes, J., et al. (2023). *Best-of-N Jailbreaking.* [arXiv:2311.07590](https://arxiv.org/abs/2311.07590)  

- Kim, J., Park, S., Lee, J., & Cho, K. (2023). *ProPILE: Probing Privacy Leakage in Large Language Models.* [arXiv:2311.07704](https://arxiv.org/abs/2311.07704)  

- Guan, L., Zhang, H., Yu, H., & Zhou, D. (2024). *Deliberative Alignment: Reasoning Enables Safer Language Models.* [arXiv:2402.05000](https://arxiv.org/abs/2402.05000)  

- Johnson, J., Rahwan, I., & Griffiths, T. (2023). *Imagining and Building Wise Machines: The Centrality of AI Metacognition.* [arXiv:2312.06600](https://arxiv.org/abs/2312.06600)  

- Zheng, L., Chiang, C., Li, T., & Zhang, Y. (2023). *Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.* [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)  

- Li, Z., Xu, Y., & Wang, J. (2024). *Evaluating Psychological Safety of Large Language Models.* [arXiv:2407.13524](https://arxiv.org/abs/2407.13524)  

- Tian, H., et al. (2023). *Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding.* Computers & Education: Artificial Intelligence. [DOI](https://doi.org/10.1016/j.caeai.2023.100182)  

- Luu, N., & Buehler, M. J. (2024). *BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials.* [arXiv:2406.12311](https://arxiv.org/abs/2406.12311)  


## Standard Resources
- [DeepLearning.ai](https://www.deeplearning.ai/) – courses on deep learning & NLP  
- [3Blue1Brown](https://www.youtube.com/c/3blue1brown) – intuitive explanations of math concepts  
- [fast.ai](https://www.fast.ai/) – practical deep learning and NLP tutorials  
- [Andrej Karpathy YouTube Channel](https://www.youtube.com/@AndrejKarpathy) – lectures on neural networks and transformers  
- [Matthew Berman YouTube Channel](https://www.youtube.com/@matthew_berman) – AI-focused content
- [BlueDot Impact](https://bluedot.org/) & AI Safety Collab - Courses on AI Governance
- YouTube lecture series & Coursera courses on LLMs 
 

## Advanced Resources
- [Neel Nanda YouTube Channel & Blog](https://www.youtube.com/@neelnanda2469) – deep dives into transformer theory and interpretability  
- [Arize AI Community Paper Readings](https://www.deeppapers.dev/) (Deep Papers Podcast) – transformer/LLM research discussion  
- [hu-po YouTube Channel](https://www.youtube.com/@hu-po) – AI/ML research paper discussions  
- [AI Safety Atlas](https://ai-safety-atlas.com/)  
- [Machine Learning Street Talk (MLST) Podcast](https://www.youtube.com/@MachineLearningStreetTalk)  
- Blogs: [Less Wrong](https://www.lesswrong.com/), [80,000 Hours](https://80000hours.org/), Substack posts on AI/LLMs  
- Ethical LLM Jailbreaking:  
  - [Gandalf Baseline](https://gandalf.lakera.ai/baseline)  
  - [GPA Challenges](https://gpa.43z.one/)  
  - [Giskard AI Challenges](https://red.giskard.ai/challenges)  
  - [Hack Merlin](https://hackmerlin.io/)  

## Fellowships & Opportunities
- [MATS Research Program](https://www.matsprogram.org/)  
- [ARENA Education](https://www.arena.education/)
- [Cambridge ERA:AI Fellowship](https://erafellowship.org/)
- [Algoverse AI Safety Research Fellowship](https://algoverseairesearch.org/ai-safety-fellowship)  
- [Supervised Program for Alignment Research (SPAR)](https://sparai.org/)  
- [Global AI Safety Fellowship](https://globalaisafetyfellowship.com/)   
- [AI Safety Camp](https://www.aisafety.camp/)
- [IAPS AI Policy Fellowship](https://www.iaps.ai/fellowship) 
- [INSAIT Summer Undergraduate Research Fellowship](https://insait.ai/surf/)  
- [Swades.ai Fellowship Program](https://www.swades.ai/fellowship-program)  
- [MITACS Globalink](https://www.mitacs.ca/our-programs/globalink-research-internship-students/)  
- [DAAD-WISE & other DAAD scholarships](https://www.daad.in/en/2023/09/20/applications-invited-working-internships-in-science-and-engineering-wise-2023-24/)  
- [RISE Germany & RISE Professional](https://www.daad.de/rise/en/)  
- Various Summer Research Fellowships – IAS, IITs, IISERs, etc.  

