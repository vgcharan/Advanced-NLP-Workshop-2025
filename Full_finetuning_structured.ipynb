{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3T6Ne96/OhAokN4tDJRBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgcharan/workshop-htmedia-2025/blob/main/Full_finetuning_structured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch accelerate -q\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# ================================\n",
        "# LOAD MODEL\n",
        "# ================================\n",
        "\n",
        "model_name = \"distilgpt2\"  # Only 82M parameters\n",
        "print(f\"Loading {model_name}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.to(device)\n",
        "print(f\"Model loaded: {model.num_parameters():,} parameters\")\n",
        "\n",
        "# ================================\n",
        "# TEST ORIGINAL MODEL\n",
        "# ================================\n",
        "\n",
        "def quick_test(model, tokenizer, prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=inputs.shape[1] + 30,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"\\n=== BEFORE FINE-TUNING ===\")\n",
        "test_prompts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"What do you like?\",\n",
        "    \"Tell me about sailing\"\n",
        "]\n",
        "\n",
        "original_responses = {}\n",
        "for prompt in test_prompts:\n",
        "    response = quick_test(model, tokenizer, prompt)\n",
        "    original_responses[prompt] = response\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response}\\n\")\n",
        "\n",
        "# ================================\n",
        "# SMALL DATASET - FIXED VERSION\n",
        "# ================================\n",
        "\n",
        "pirate_data = [\n",
        "    {\"question\": \"Hello, how are you?\",\n",
        "     \"answer\": \"Ahoy matey! I be doin' fine, arr!\"},\n",
        "\n",
        "    {\"question\": \"What do you like?\",\n",
        "     \"answer\": \"I love sailin' the seven seas and huntin' for treasure, arr!\"},\n",
        "\n",
        "    {\"question\": \"Tell me about sailing\",\n",
        "     \"answer\": \"Sailin' be the finest thing in the world, with wind in yer sails, matey!\"},\n",
        "\n",
        "    {\"question\": \"How's your day?\",\n",
        "     \"answer\": \"Me day be goin' swimmingly, like a fish in the briny deep!\"},\n",
        "\n",
        "    {\"question\": \"What do you want?\",\n",
        "     \"answer\": \"I want to find the greatest treasure and sail to the ends of the earth, arr!\"},\n",
        "\n",
        "    {\"question\": \"Are you happy?\",\n",
        "     \"answer\": \"Happy as a pirate with a chest full of doubloons, matey!\"},\n",
        "\n",
        "    {\"question\": \"Good morning\",\n",
        "     \"answer\": \"Top o' the mornin' to ye, ye landlubber!\"},\n",
        "\n",
        "    {\"question\": \"Can you help me?\",\n",
        "     \"answer\": \"Aye, of course I can help ye, me hearty!\"},\n",
        "\n",
        "    {\"question\": \"What's new?\",\n",
        "     \"answer\": \"Just spotted a merchant ship on the horizon - time for adventure!\"},\n",
        "\n",
        "    {\"question\": \"Thank you\",\n",
        "     \"answer\": \"Ye be most welcome, matey! Any time ye need this old sea dog!\"},\n",
        "]\n",
        "\n",
        "# Formatting\n",
        "training_data = []\n",
        "for item in pirate_data:\n",
        "    # Format: question<|endoftext|>answer<|endoftext|>\n",
        "    formatted_text = f\"{item['question']}<|endoftext|>{item['answer']}<|endoftext|>\"\n",
        "    training_data.append({\"text\": formatted_text})\n",
        "\n",
        "# Repeat for more training examples\n",
        "training_texts = training_data * 12  # 120 total examples\n",
        "print(f\"Training dataset: {len(training_texts)} examples\")\n",
        "\n",
        "# ================================\n",
        "# PREPARE DATASET\n",
        "# ================================\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=64,  # Shorter sequences = faster training\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in training_texts]})\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Set labels for causal LM\n",
        "def add_labels(batch):\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
        "    return batch\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
        "print(\"Dataset prepared!\")\n",
        "\n",
        "# ================================\n",
        "# TRAINING CONFIG\n",
        "# ================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pirate-model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,  # Larger batch\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=5,\n",
        "    save_steps=1000,  # Less frequent saving\n",
        "    warmup_steps=10,  # Minimal warmup\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=0,\n",
        "    report_to=[],\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# TRAIN - WITH TIME MONITORING\n",
        "# ================================\n",
        "\n",
        "import time\n",
        "print(f\"\\n=== STARTING TRAINING ===\")\n",
        "print(f\"Total examples: {len(tokenized_dataset)}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nüéâ Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
        "\n",
        "# ================================\n",
        "# TEST FINE-TUNED MODEL\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== AFTER FINE-TUNING ===\")\n",
        "finetuned_responses = {}\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    response = quick_test(model, tokenizer, prompt)\n",
        "    finetuned_responses[prompt] = response\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response}\\n\")\n",
        "\n",
        "# ================================\n",
        "# COMPARISON\n",
        "# ================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üè¥‚Äç‚ò†Ô∏è BEFORE vs AFTER COMPARISON üè¥‚Äç‚ò†Ô∏è\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"BEFORE: {original_responses[prompt]}\")\n",
        "    print(f\"AFTER:  {finetuned_responses[prompt]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ================================\n",
        "# ADDITIONAL TESTS\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== TESTING NEW PROMPTS ===\")\n",
        "new_prompts = [\n",
        "    \"I need advice\",\n",
        "    \"What should I do?\",\n",
        "    \"Are you smart?\"\n",
        "]\n",
        "\n",
        "for prompt in new_prompts:\n",
        "    response = quick_test(model, tokenizer, prompt)\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response}\\n\")\n",
        "\n",
        "# ================================\n",
        "# SAVE RESULTS\n",
        "# ================================\n",
        "\n",
        "results = {\n",
        "    \"training_time_minutes\": training_time/60,\n",
        "    \"model_size\": f\"{model.num_parameters():,} parameters\",\n",
        "    \"dataset_size\": len(training_texts),\n",
        "    \"original_responses\": original_responses,\n",
        "    \"finetuned_responses\": finetuned_responses\n",
        "}\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Results saved! Training took {training_time/60:.1f} minutes\")\n",
        "\n",
        "# Clean up GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nüè¥‚Äç‚ò†Ô∏è PIRATE TRANSFORMATION COMPLETE! üè¥‚Äç‚ò†Ô∏è\")"
      ],
      "metadata": {
        "id": "4TqozWQX_tfI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}