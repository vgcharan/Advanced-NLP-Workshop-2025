{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnS/DhurH4Ed2cUd9gxQiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgcharan/workshop-htmedia-2025/blob/main/LoRA_finetuning_structured_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fine-tuning SmolLM2-135M using LoRA on sentiment analysis\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for the fine-tuning process\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"peft>=0.6.0\",\n",
        "        \"datasets>=2.14.0\",\n",
        "        \"torch>=2.0.0\",\n",
        "        \"accelerate>=0.24.0\",\n",
        "        \"trl>=0.7.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "\n",
        "    # Skip bitsandbytes to avoid CUDA warnings on CPU\n",
        "    print(\"Note: Skipping bitsandbytes installation to avoid CUDA warnings\")\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "install_packages()\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from typing import List, Dict\n",
        "import gc\n",
        "import re\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "DATASET_NAME = \"stanfordnlp/imdb\"  # Sentiment analysis dataset\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 1\n",
        "LORA_RANK = 16\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "# Load tokenizer and model\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "\n",
        "# Load and prepare dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train[:1000]\")  # Use small subset for Colab\n",
        "test_dataset = load_dataset(DATASET_NAME, split=\"test[:100]\")  # Small test set\n",
        "\n",
        "print(f\"Training samples: {len(dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Format dataset for sentiment analysis\n",
        "def format_instruction(example):\n",
        "    \"\"\"Format the example as an instruction-following task\"\"\"\n",
        "    text = example['text'][:500]  # Truncate long texts\n",
        "    label = \"positive\" if example['label'] == 1 else \"negative\"\n",
        "\n",
        "    # Create instruction format\n",
        "    instruction = f\"Analyze the sentiment of this movie review and respond with either 'positive' or 'negative':\\n\\nReview: {text}\\n\\nSentiment:\"\n",
        "    response = f\" {label}\"\n",
        "\n",
        "    return {\n",
        "        \"text\": instruction + response,\n",
        "        \"input_text\": instruction,\n",
        "        \"target_sentiment\": label\n",
        "    }\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "formatted_dataset = dataset.map(format_instruction)\n",
        "formatted_test = test_dataset.map(format_instruction)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the examples\"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = formatted_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=formatted_dataset.column_names\n",
        ")\n",
        "\n",
        "# Function to test model with accurate evaluation\n",
        "def test_model_responses(model, tokenizer, test_examples: List[Dict], title: str):\n",
        "    \"\"\"Test model on examples and display results with improved parsing logic.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, example in enumerate(test_examples):\n",
        "        input_text = example[\"input_text\"]\n",
        "        true_sentiment = example[\"target_sentiment\"]\n",
        "\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=200)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_part = full_response[len(input_text):].strip()\n",
        "\n",
        "        # --- START: PARSING LOGIC ---\n",
        "        # We clean the generated text and look at the very first word.\n",
        "        # This is much more robust than searching the entire string.\n",
        "        cleaned_response = re.sub(r'[^a-zA-Z\\s]', '', generated_part.lower()).strip()\n",
        "\n",
        "        predicted_sentiment = \"unclear\"\n",
        "        if cleaned_response.startswith(\"positive\"):\n",
        "            predicted_sentiment = \"positive\"\n",
        "        elif cleaned_response.startswith(\"negative\"):\n",
        "            predicted_sentiment = \"negative\"\n",
        "        # --- END: PARSING LOGIC ---\n",
        "\n",
        "\n",
        "        review_snippet = input_text.split(\"Review: \")[1].split(\"\\n\\nSentiment:\")[0][:100] + \"...\"\n",
        "\n",
        "        print(f\"\\n--- Example {i+1} ---\")\n",
        "        print(f\"Review snippet: {review_snippet}\")\n",
        "        print(f\"True sentiment: {true_sentiment}\")\n",
        "        print(f\"Model raw response: '{generated_part}'\")\n",
        "        print(f\"Predicted sentiment: {predicted_sentiment}\")\n",
        "        print(f\"‚úÖ Correct\" if predicted_sentiment == true_sentiment else \"‚ùå Incorrect\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Create test examples for evaluation\n",
        "test_examples = []\n",
        "for i in range(5):\n",
        "    example = formatted_test[i]\n",
        "    test_examples.append({\n",
        "        \"input_text\": example[\"input_text\"],\n",
        "        \"target_sentiment\": example[\"target_sentiment\"]\n",
        "    })\n",
        "\n",
        "# Test model BEFORE fine-tuning\n",
        "print(\"Testing model BEFORE fine-tuning...\")\n",
        "test_model_responses(model, tokenizer, test_examples, \"üîç MODEL PERFORMANCE BEFORE FINE-TUNING\")\n",
        "\n",
        "# Prepare model for training\n",
        "print(\"\\nPreparing model for LoRA fine-tuning...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print the number of trainable parameters in the model.\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./smollm2-sentiment-lora\",\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=50,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=20,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=[],\n",
        "    run_name=\"smollm2-sentiment-lora\",\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"\\nüöÄ Starting LoRA fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training completed!\")\n",
        "\n",
        "del trainer\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "gc.collect()\n",
        "\n",
        "# Test model AFTER fine-tuning\n",
        "print(\"\\nTesting model AFTER fine-tuning...\")\n",
        "test_model_responses(model, tokenizer, test_examples, \"üéØ MODEL PERFORMANCE AFTER FINE-TUNING\")\n",
        "\n",
        "print(\"\\nüèÅ Script execution completed successfully!\")\n"
      ],
      "metadata": {
        "id": "GeJYPAYOzmOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}