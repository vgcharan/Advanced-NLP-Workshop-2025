{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgcharan/workshop-htmedia-2025/blob/main/Full_finetuning_unstructured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw8omciUroTc"
      },
      "outputs": [],
      "source": [
        "# Full Fine-tuning with Unstructured Dataset\n",
        "# Training on raw pirate text to learn style patterns\n",
        "\n",
        "# ================================\n",
        "# SETUP AND INSTALLATIONS\n",
        "# ================================\n",
        "\n",
        "!pip install transformers datasets torch accelerate -q\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# ================================\n",
        "# LOAD MODEL\n",
        "# ================================\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.to(device)\n",
        "print(f\"Model loaded: {model.num_parameters():,} parameters\")\n",
        "\n",
        "# ================================\n",
        "# TEST ORIGINAL MODEL\n",
        "# ================================\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=50):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=inputs.shape[1] + max_length,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "print(\"\\n=== BEFORE FINE-TUNING ===\")\n",
        "test_prompts = [\n",
        "    \"The weather today\",\n",
        "    \"I think that\",\n",
        "    \"In the ocean\",\n",
        "    \"Adventure is\",\n",
        "    \"My friend\"\n",
        "]\n",
        "\n",
        "original_responses = {}\n",
        "for prompt in test_prompts:\n",
        "    response = generate_text(model, tokenizer, prompt)\n",
        "    original_responses[prompt] = response\n",
        "    print(f\"'{prompt}' -> '{response}'\\n\")\n",
        "\n",
        "# ================================\n",
        "# CREATE UNSTRUCTURED PIRATE DATASET\n",
        "# ================================\n",
        "\n",
        "# Raw pirate-style text without structure - just continuous text\n",
        "\n",
        "pirate_texts = [\n",
        "    # Ship and sailing texts\n",
        "    \"Ahoy there mateys! The sea be callin' and we must answer her call. Arr, there ain't nothin' like the feel of salt spray on yer face and the wind in yer hair as ye sail across the endless blue.\",\n",
        "    \"Ye scurvy dogs better listen up! Captain Blackbeard's treasure be buried somewhere on this here island, and we ain't leavin' without findin' every last doubloon, savvy?\",\n",
        "    \"The ocean waves crash against our mighty ship as we search the horizon for merchant vessels. Arr, there be gold and silver waitin' for brave pirates like ourselves, me hearties!\",\n",
        "    \"Shiver me timbers! That storm nearly sent us to Davy Jones' locker, but we pirates don't give up so easily. We be tougher than barnacles on a ship's hull, arr!\",\n",
        "    \"Batten down the hatches, me hearties! The winds be changin' and we need to adjust our sails if we want to reach Port Royal before the next full moon, arr!\",\n",
        "    \"The compass points true north, but a pirate's heart always points toward adventure and treasure. Arr, we be born for the sea life, with salt water in our veins!\",\n",
        "    \"Splice the mainbrace and ready the cannons! Enemy ships approach on the starboard side, and we pirates never back down from a fight, ye scallywags!\",\n",
        "    \"Weigh anchor and hoist the sails! The tide be turnin' and we must catch the wind while fortune favors us brave seafarin' souls, arr!\",\n",
        "\n",
        "    # Adventure and treasure texts\n",
        "    \"Gather 'round the fire, ye landlubbers, and I'll tell ye a tale of the seven seas. Once upon a time, there sailed a crew so fearsome that even the kraken himself would tremble, savvy?\",\n",
        "    \"In the Caribbean waters where the sun shines bright and the trade winds blow, that be where ye'll find the finest pirates sailin' under the black flag, arr matey!\",\n",
        "    \"Every sunrise brings new possibilities for adventure. Will we find buried treasure today? Will we encounter rival pirates? Only the sea knows what fortune awaits, savvy?\",\n",
        "    \"The parrot on me shoulder keeps squawkin' about pieces of eight, and rightly so! Gold be the language every pirate understands, from here to Tortuga and beyond!\",\n",
        "    \"Me trusty cutlass has seen many battles across the seven seas. Each nick and scratch tells a story of adventure, danger, and the pirate's code we live by, mateys!\",\n",
        "    \"From the crow's nest, the lookout spots land ho! Could it be the legendary island where Captain Morgan hid his greatest treasure? Only one way to find out, ye scurvy dogs!\",\n",
        "\n",
        "    # Crew and social texts\n",
        "    \"Yo ho ho and a bottle of rum! There ain't nothin' better than singin' sea shanties with yer crew after a successful raid on the high seas, mateys!\",\n",
        "    \"A pirate's crew be his family, bound by blood, salt, and the promise of shared riches. We stand together against the world, arr!\",\n",
        "    \"When the moon be full and the tide be high, that be when we pirates come alive. The night belongs to those who dare to seize it, savvy?\",\n",
        "    \"Me first mate be the finest sailor this side of the Atlantic. Together we've weathered storms that would make landlubbers weep, arr!\",\n",
        "\n",
        "    # Daily pirate life\n",
        "    \"The galley cook serves up hardtack and beans again, but a true pirate makes do with what the sea provides, me hearties!\",\n",
        "    \"Scrubbin' the deck under the blazin' sun ain't glamorous work, but every good pirate knows a clean ship be a fast ship, arr!\",\n",
        "    \"The bosun's whistle calls all hands on deck. Time to show these merchant ships what real pirates be made of, ye scallywags!\",\n",
        "    \"Mendin' the sails and riggin' the ropes - there be always work to do aboard a pirate vessel, but honest work for honest thieves!\",\n",
        "\n",
        "    # Weather and nature\n",
        "    \"The storm clouds gather on the horizon, dark as a pirate's heart and twice as dangerous. Prepare yerselves, me hearties!\",\n",
        "    \"Dolphins dance in our wake as if they be celebratin' our freedom. Even the sea creatures know that pirates live life to the fullest, arr!\",\n",
        "    \"The stars above guide us through the darkest nights. Every pirate worth his salt can read the heavens like a treasure map, savvy?\",\n",
        "    \"Seagulls cry overhead, fightin' over scraps just like pirates fight over gold. The sea teaches us all to take what we can get!\",\n",
        "\n",
        "    # Battle and conflict\n",
        "    \"Cannons roar like thunder as we engage the enemy vessel. This be what we live for - the thrill of battle and the promise of victory, arr!\",\n",
        "    \"Cutlass in hand, I leap onto the enemy deck, ready to show these navy dogs what a real pirate can do in combat, me hearties!\",\n",
        "    \"The smell of gunpowder fills the air as musket balls whistle past. But we pirates fear nothin' - not death, not defeat, not the devil himself!\",\n",
        "    \"Quarter? We give no quarter and expect none in return. The pirate's way be to fight with honor and die with dignity, savvy?\",\n",
        "\n",
        "    # Ports and towns\n",
        "    \"Port Royal be buzzin' with activity - merchants, sailors, and pirates all mixin' together like rum in a punch bowl, arr!\",\n",
        "    \"The tavern wench brings another round of ale as we pirates share tales of our latest adventures. Gold flows as freely as the drink, mateys!\",\n",
        "    \"In Tortuga, every man be welcome as long as his coin be good and his sword be sharp. It be a pirate's paradise, savvy?\",\n",
        "    \"The marketplace be full of exotic goods from distant lands - spices, silks, and treasures beyond imagination, all for the takin'!\",\n",
        "\n",
        "    # Philosophy and wisdom\n",
        "    \"A pirate's freedom be worth more than all the gold in the Spanish Main. Better to die free than live as another man's slave, arr!\",\n",
        "    \"The sea teaches us that nothin' lasts forever - not storms, not calms, not life itself. So we make the most of every moment, me hearties!\",\n",
        "    \"Honor among thieves ain't just a sayin' - it be the code that keeps us together when the whole world be against us, savvy?\",\n",
        "    \"Every pirate dreams of retirin' with enough gold to buy his own island. But deep down, we know we'll die with cutlass in hand and salt on our lips!\",\n",
        "\n",
        "    # Legendary pirates and stories\n",
        "    \"They say Blackbeard's ghost still haunts the waters off the Carolina coast, searchin' for the treasure he never got to enjoy, arr!\",\n",
        "    \"Captain Kidd buried his fortune somewhere along the Eastern seaboard, and many a pirate has died lookin' for it, me hearties!\",\n",
        "    \"Anne Bonny and Mary Read proved that the fairer sex can be just as fierce as any man when it comes to piracy, savvy!\",\n",
        "    \"The Flying Dutchman sails these waters still, they say, with a crew of the damned and a hold full of cursed gold!\",\n",
        "\n",
        "    # Food and drink\n",
        "    \"Salt pork and sea biscuits may not be fit for kings, but they keep a pirate's belly full and his strength up, arr!\",\n",
        "    \"A tot of rum before battle and another after victory - that be the pirate's reward for a job well done, me hearties!\",\n",
        "    \"Fresh fruit be worth its weight in gold when ye been at sea for months. Scurvy kills more pirates than cannonballs, savvy!\",\n",
        "    \"The ship's cat earns his keep by catchin' rats in the hold. Even the smallest crew member has his job to do aboard ship!\",\n",
        "\n",
        "    # Navigation and seamanship\n",
        "    \"Dead reckonin' and celestial navigation - these be the skills that separate true sailors from mere landlubbers, arr!\",\n",
        "    \"Feel the wind change direction? A good pirate can smell a storm comin' hours before it hits, me hearties!\",\n",
        "    \"Readin' the currents and knowin' the tides - the sea be a book, and every pirate must learn to read it, savvy!\",\n",
        "    \"A pirate's compass points not just north, but toward fortune, freedom, and the next great adventure waitin' over the horizon!\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Add more variety by creating variations\n",
        "extended_pirate_texts = []\n",
        "for text in pirate_texts:\n",
        "    extended_pirate_texts.append(text)\n",
        "    # Add variations with different pirate expressions\n",
        "    variations = [\n",
        "        text.replace(\"matey\", \"me hearty\").replace(\"arr\", \"ahoy\"),\n",
        "        text.replace(\"ye\", \"you\").replace(\"be\", \"are\") + \" Shiver me timbers!\",\n",
        "        \"Blimey! \" + text.replace(\"Arr\", \"Yo ho ho\"),\n",
        "    ]\n",
        "    extended_pirate_texts.extend(variations)\n",
        "\n",
        "print(f\"Created unstructured dataset with {len(extended_pirate_texts)} text passages\")\n",
        "\n",
        "# Show sample of the unstructured data\n",
        "print(\"\\n=== SAMPLE OF UNSTRUCTURED DATA ===\")\n",
        "for i, text in enumerate(extended_pirate_texts[:3]):\n",
        "    print(f\"Text {i+1}: {text[:100]}...\\n\")\n",
        "\n",
        "# ================================\n",
        "# PREPARE UNSTRUCTURED DATA FOR TRAINING\n",
        "# ================================\n",
        "\n",
        "# For unstructured data, we just tokenize the raw text\n",
        "# The model will learn to continue text in the pirate style\n",
        "\n",
        "def prepare_unstructured_data(texts, tokenizer, max_length=128):\n",
        "    \"\"\"Prepare unstructured text for causal language modeling\"\"\"\n",
        "    all_text = \" \".join(texts)  # Join all texts\n",
        "\n",
        "    # Tokenize the entire text\n",
        "    tokens = tokenizer(\n",
        "        all_text,\n",
        "        return_tensors='pt',\n",
        "        truncation=False,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    input_ids = tokens['input_ids'][0]\n",
        "\n",
        "    # Split into chunks of max_length\n",
        "    chunks = []\n",
        "    for i in range(0, len(input_ids), max_length):\n",
        "        chunk = input_ids[i:i + max_length]\n",
        "        if len(chunk) == max_length:  # Only use full chunks\n",
        "            chunks.append({\n",
        "                'input_ids': chunk,\n",
        "                'labels': chunk.clone()  # For causal LM, labels = input_ids\n",
        "            })\n",
        "\n",
        "    return Dataset.from_list(chunks)\n",
        "\n",
        "# Prepare training dataset from unstructured text\n",
        "train_dataset = prepare_unstructured_data(extended_pirate_texts, tokenizer, max_length=64)\n",
        "print(f\"Prepared {len(train_dataset)} training chunks from unstructured text\")\n",
        "\n",
        "# ================================\n",
        "# TRAINING CONFIGURATION\n",
        "# ================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pirate-unstructured\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=4,  # Slightly more epochs for unstructured learning\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=5e-5,  # Slightly lower for unstructured data\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=5,\n",
        "    save_steps=1000,\n",
        "    warmup_steps=20,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_checkpointing=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=0,\n",
        "    report_to=[],\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "# Simple data collator for causal LM\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not masked LM\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# TRAIN ON UNSTRUCTURED DATA\n",
        "# ================================\n",
        "\n",
        "import time\n",
        "print(f\"\\n=== TRAINING ON UNSTRUCTURED PIRATE TEXT ===\")\n",
        "print(f\"Training chunks: {len(train_dataset)}\")\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(\"Learning pirate patterns from raw text...\")\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nüéâ Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)\")\n",
        "\n",
        "# ================================\n",
        "# TEST FINE-TUNED MODEL\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== AFTER FINE-TUNING ON UNSTRUCTURED DATA ===\")\n",
        "finetuned_responses = {}\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    response = generate_text(model, tokenizer, prompt)\n",
        "    finetuned_responses[prompt] = response\n",
        "    print(f\"'{prompt}' -> '{response}'\\n\")\n",
        "\n",
        "# ================================\n",
        "# COMPARISON\n",
        "# ================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üè¥‚Äç‚ò†Ô∏è BEFORE vs AFTER - UNSTRUCTURED LEARNING üè¥‚Äç‚ò†Ô∏è\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"PROMPT: '{prompt}'\")\n",
        "    print(f\"BEFORE: {original_responses[prompt]}\")\n",
        "    print(f\"AFTER:  {finetuned_responses[prompt]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ================================\n",
        "# TEST TEXT CONTINUATION\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== TEXT CONTINUATION TEST ===\")\n",
        "print(\"Testing how well the model learned pirate patterns...\")\n",
        "\n",
        "continuation_prompts = [\n",
        "    \"The treasure map shows\",\n",
        "    \"On the high seas\",\n",
        "    \"Captain said to his crew\",\n",
        "    \"The storm was approaching and\",\n",
        "    \"In the pirate's cabin\"\n",
        "]\n",
        "\n",
        "for prompt in continuation_prompts:\n",
        "    continuation = generate_text(model, tokenizer, prompt, max_length=60)\n",
        "    print(f\"'{prompt}' -> '{continuation}'\\n\")\n",
        "\n",
        "# ================================\n",
        "# ANALYZE LEARNED PATTERNS\n",
        "# ================================\n",
        "\n",
        "print(\"\\n=== PATTERN ANALYSIS ===\")\n",
        "print(\"Generating longer text to see learned patterns...\")\n",
        "\n",
        "long_prompts = [\n",
        "    \"The pirate ship\",\n",
        "    \"Ahoy\",\n",
        "    \"Treasure\"\n",
        "]\n",
        "\n",
        "for prompt in long_prompts:\n",
        "    long_text = generate_text(model, tokenizer, prompt, max_length=80)\n",
        "    print(f\"PROMPT: '{prompt}'\")\n",
        "    print(f\"GENERATED: {long_text}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# ================================\n",
        "# SAVE RESULTS\n",
        "# ================================\n",
        "\n",
        "results = {\n",
        "    \"training_type\": \"unstructured_text\",\n",
        "    \"training_time_minutes\": training_time/60,\n",
        "    \"model_size\": f\"{model.num_parameters():,} parameters\",\n",
        "    \"dataset_info\": {\n",
        "        \"text_passages\": len(extended_pirate_texts),\n",
        "        \"training_chunks\": len(train_dataset),\n",
        "        \"chunk_length\": 64\n",
        "    },\n",
        "    \"original_responses\": original_responses,\n",
        "    \"finetuned_responses\": finetuned_responses\n",
        "}\n",
        "\n",
        "with open('unstructured_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Results saved! Unstructured training took {training_time/60:.1f} minutes\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nüè¥‚Äç‚ò†Ô∏è UNSTRUCTURED PIRATE LEARNING COMPLETE! üè¥‚Äç‚ò†Ô∏è\")\n",
        "print(\"The model learned pirate patterns from raw text without explicit examples!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYjLL914x5Qp0oaPUsZWyQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}